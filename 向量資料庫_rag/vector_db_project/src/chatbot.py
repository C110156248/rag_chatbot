import os
import streamlit as st
from dotenv import load_dotenv
from data_processing import process_document, split_text
from llama_index.core import VectorStoreIndex, StorageContext, Document, Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.llms.gemini import Gemini
import chromadb  

# è¨­å®šé é¢é…ç½®
st.set_page_config(page_title="éœ‡å‹•åˆ†æèŠå¤©æ©Ÿå™¨äºº", layout="wide")

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
GEMINI_API = os.getenv("gemini_key")
file_path = os.getenv("file_path")
persist_dir = os.getenv("persist_dir")
model_name = os.getenv("model_name")
collection_name = os.getenv("collection_name")

# åˆå§‹åŒ– session state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "query_engine" not in st.session_state:
    # è¨­å®šåµŒå…¥æ¨¡å‹
    embed_model = GeminiEmbedding(
        api_key=GEMINI_API,
        model_name=model_name
    )
    Settings.embed_model = embed_model

    # è¨­å®š LLM æ¨¡å‹
    llm = Gemini(api_key=GEMINI_API)
    Settings.llm = llm

    # è¼‰å…¥å’Œè™•ç†æ–‡ä»¶
    text_content = process_document(file_path)
    text_chunks = split_text(text_content)
    documents = [Document(text=chunk) for chunk in text_chunks]

    # è¨­å®šå‘é‡å„²å­˜
    chroma_client = chromadb.PersistentClient(path=persist_dir)
    chroma_collection = chroma_client.get_or_create_collection("my_collection")
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # å»ºç«‹ç´¢å¼•å’ŒæŸ¥è©¢å¼•æ“
    index = VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
        embed_model=embed_model,
    )
    st.session_state.query_engine = index.as_query_engine()

# é¡¯ç¤ºæ¨™é¡Œ
st.title("éœ‡å‹•åˆ†æèŠå¤©æ©Ÿå™¨äºº ğŸ¤–")
st.markdown("---")

# é¡¯ç¤ºèŠå¤©è¨Šæ¯æ­·å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# èŠå¤©è¼¸å…¥
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ"):
    # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # é¡¯ç¤ºåŠ©ç†å›æ‡‰
    with st.chat_message("assistant"):
        response = st.session_state.query_engine.query(prompt)
        st.markdown(response.response)
    st.session_state.messages.append({"role": "assistant", "content": response.response})

# é¡¯ç¤ºä½¿ç”¨èªªæ˜
with st.sidebar:
    st.markdown("### ä½¿ç”¨èªªæ˜")
    st.markdown("""
    1. åœ¨è¼¸å…¥æ¡†ä¸­è¼¸å…¥æ‚¨çš„å•é¡Œ
    2. ç³»çµ±æœƒå¾éœ‡å‹•åˆ†æè³‡æ–™åº«ä¸­å°‹æ‰¾ç›¸é—œç­”æ¡ˆ
    3. æ‚¨å¯ä»¥ç¹¼çºŒæå•ï¼Œç³»çµ±æœƒä¿æŒå°è©±ç´€éŒ„
    """)

#streamlit run ./å‘é‡è³‡æ–™åº«_rag/vector-db-project/src/chatbot.py